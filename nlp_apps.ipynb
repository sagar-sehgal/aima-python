{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING APPLICATIONS\n",
    "\n",
    "In this notebook we will take a look at some indicative applications of natural language processing. We will cover content from [`nlp.py`](https://github.com/aimacode/aima-python/blob/master/nlp.py) and [`text.py`](https://github.com/aimacode/aima-python/blob/master/text.py), for chapters 22 and 23 of Stuart Russel's and Peter Norvig's book [*Artificial Intelligence: A Modern Approach*](http://aima.cs.berkeley.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "* Language Recognition\n",
    "* Author Recognition\n",
    "* The Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE RECOGNITION\n",
    "\n",
    "A very useful application of text models (you can read more on them on the [`text notebook`](https://github.com/aimacode/aima-python/blob/master/text.ipynb)) is categorizing text into a language. In fact, with enough data we can categorize correctly mostly any text. That is because different languages have certain characteristics that set them apart. For example, in German it is very usual for 'c' to be followed by 'h' while in English we see 't' followed by 'h' a lot.\n",
    "\n",
    "Here we will build an application to categorize sentences in either English or German.\n",
    "\n",
    "First we need to build our dataset. We will take as input text in English and in German and we will extract n-gram character models (in this case, *bigrams* for n=2). For English, we will use *Flatland* by Edwin Abbott and for German *Faust* by Goethe.\n",
    "\n",
    "Let's build our text models for each language, which will hold the probability of each bigram occuring in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import open_data\n",
    "from text import *\n",
    "\n",
    "flatland = open_data(\"EN-text/flatland.txt\").read()\n",
    "wordseq = words(flatland)\n",
    "\n",
    "P_flatland = NgramCharModel(2, wordseq)\n",
    "\n",
    "faust = open_data(\"GE-text/faust.txt\").read()\n",
    "wordseq = words(faust)\n",
    "\n",
    "P_faust = NgramCharModel(2, wordseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to build a *Naive Bayes Classifier* that will be used to categorize sentences (you can read more on Naive Bayes on the [`learning notebook`](https://github.com/aimacode/aima-python/blob/master/learning.ipynb)). The classifier will take as input the probability distribution of bigrams and given a list of bigrams (extracted from the sentence to be classified), it will calculate the probability of the example/sentence coming from each language and pick the maximum.\n",
    "\n",
    "Let's build our classifier, with the assumption that English is as probable as German (the input is a dictionary with values the text models and keys the tuple `language, probability`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from learning import NaiveBayesLearner\n",
    "\n",
    "dist = {('English', 1): P_flatland, ('German', 1): P_faust}\n",
    "\n",
    "nBS = NaiveBayesLearner(dist, simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write a function that takes as input a sentence, breaks it into a list of bigrams and classifies it with the naive bayes classifier from above.\n",
    "\n",
    "Once we get the text model for the sentence, we need to unravel it. The text models show the probability of each bigram, but the classifier can't handle that extra data. It requires a simple *list* of bigrams. So, if the text model shows that a bigram appears three times, we need to add it three times in the list. Since the text model stores the n-gram information in a dictionary (with the key being the n-gram and the value the number of times the n-gram appears) we need to iterate through the items of the dictionary and manually add them to the list of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize(sentence, nBS, n):\n",
    "    sentence = sentence.lower()\n",
    "    wordseq = words(sentence)\n",
    "    \n",
    "    P_sentence = NgramCharModel(n, wordseq)\n",
    "    \n",
    "    ngrams = []\n",
    "    for b, p in P_sentence.dictionary.items():\n",
    "        ngrams += [b]*p\n",
    "    \n",
    "    print(ngrams)\n",
    "    \n",
    "    return nBS(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start categorizing sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 'i'), ('i', 'c'), ('c', 'h'), (' ', 'b'), ('b', 'i'), ('i', 'n'), ('i', 'n'), (' ', 'e'), ('e', 'i'), (' ', 'p'), ('p', 'l'), ('l', 'a'), ('a', 't'), ('t', 'z')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"Ich bin ein platz\", nBS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 't'), ('t', 'u'), ('u', 'r'), ('r', 't'), ('t', 'l'), ('l', 'e'), ('e', 's'), (' ', 'f'), ('f', 'l'), ('l', 'y'), (' ', 'h'), ('h', 'i'), ('i', 'g'), ('g', 'h')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"Turtles fly high\", nBS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 'd'), ('d', 'e'), ('e', 'r'), ('e', 'r'), (' ', 'p'), ('p', 'e'), ('e', 'l'), ('l', 'i'), ('i', 'k'), ('k', 'a'), ('a', 'n'), (' ', 'i'), ('i', 's'), ('s', 't'), (' ', 'h'), ('h', 'i'), ('i', 'e')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"Der pelikan ist hier\", nBS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 'a'), ('a', 'n'), ('n', 'd'), (' ', 't'), (' ', 't'), ('t', 'h'), ('t', 'h'), ('h', 'u'), ('u', 's'), ('h', 'e'), (' ', 'w'), ('w', 'i'), ('i', 'z'), ('z', 'a'), ('a', 'r'), ('r', 'd'), (' ', 's'), ('s', 'p'), ('p', 'o'), ('o', 'k'), ('k', 'e')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"And thus the wizard spoke\", nBS, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more languages if you want, the algorithm works for as many as you like! Also, you can play around with *n*. Here we used 2, but other numbers work too (even though 2 suffices). The algorithm is not perfect, but it has high accuracy even for small samples like the ones we used. That is because English and German are very different languages. The closer together languages are (for example, Norwegian and Swedish share a lot of common ground) the lower the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTHOR RECOGNITION\n",
    "\n",
    "Another similar application to language recognition is recognizing who is more likely to have written a sentence, given text written by them. Here we will try and predict text from Edwin Abbott and Jane Austen. They wrote *Flatland* and *Pride and Prejudice* respectively.\n",
    "\n",
    "We are optimistic we can determine who wrote what based on the fact that Abbott wrote his novella on much later date than Austen, which means there will be linguistic differences between the two works. Indeed, *Flatland* uses more modern and direct language while *Pride and Prejudice* is written in a more archaic tone containing more sophisticated wording.\n",
    "\n",
    "Similarly with Language Recognition, we will first import the two datasets. This time though we are not looking for connections between characters, since that wouldn't give that great results. Why? Because both authors use English and English follows a set of patterns, as we show earlier. Trying to determine authorship based on this patterns would not be very efficient.\n",
    "\n",
    "Instead, we will abstract our querying to a higher level. We will use words instead of characters. That way we can more accurately pick at the differences between their writing style and thus have a better chance at guessing the correct author.\n",
    "\n",
    "Let's go right ahead and import our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import open_data\n",
    "from text import *\n",
    "\n",
    "flatland = open_data(\"EN-text/flatland.txt\").read()\n",
    "wordseq = words(flatland)\n",
    "\n",
    "P_Abbott = UnigramWordModel(wordseq, 5)\n",
    "\n",
    "pride = open_data(\"EN-text/pride.txt\").read()\n",
    "wordseq = words(pride)\n",
    "\n",
    "P_Austen = UnigramWordModel(wordseq, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we set the `default` parameter of the model to 5, instead of 0. If we leave it at 0, then when we get a sentence containing a word we have not seen from that particular author, the chance of that sentence coming from that author is exactly 0 (since to get the probability, we multiply all the separate probabilities; if one is 0 then the result is also 0). To avoid that, we tell the model to add 5 to the count of all the words that appear.\n",
    "\n",
    "Next we will build the Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from learning import NaiveBayesLearner\n",
    "\n",
    "dist = {('Abbott', 1): P_Abbott, ('Austen', 1): P_Austen}\n",
    "\n",
    "nBS = NaiveBayesLearner(dist, simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have build our classifier, we will start classifying. First, we need to convert the given sentence to the format the classifier needs. That is, a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize(sentence, nBS):\n",
    "    sentence = sentence.lower()\n",
    "    sentence_words = words(sentence)\n",
    "    \n",
    "    return nBS(sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will input a sentence that is something Abbott would write. Note the use of square and the simpler language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abbott'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"the square is mad\", nBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier correctly guessed Abbott.\n",
    "\n",
    "Next we will input a more sophisticated sentence, similar to the style of Austen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austen'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize(\"a most peculiar acquaintance\", nBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier guessed correctly again.\n",
    "\n",
    "You can try more sentences on your own. Unfortunately though, since the datasets are pretty small, chances are the guesses will not always be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE FEDERALIST PAPERS\n",
    "\n",
    "Let's now take a look at a harder problem, classifying the authors of the [Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers). The *Federalist Papers* are a series of papers written by Alexander Hamilton, James Madison and John Jay towards establishing the United States Constitution.\n",
    "\n",
    "What is interesting about these papers is that they were all written under a pseudonym, \"Publius\", to keep the identity of the authors a secret. Only after Hamilton's death, when a list was found written by him detailing the authorship of the papers, did the rest of the world learn what papers each of the authors wrote. After the list was published, Madison chimed in to make a couple of corrections: Hamilton, Madison said, hastily wrote down the list and assigned some papers to the wrong author!\n",
    "\n",
    "Here we will try and find out who really wrote these mysterious papers.\n",
    "\n",
    "To solve this we will learn from the undisputed papers to predict the disputed ones. First, let's read the texts from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import open_data\n",
    "from text import *\n",
    "\n",
    "federalist = open_data(\"EN-text/federalist.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the text looks. We will print the first 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Federalist Papers, by \\nAlexander Hamilton and John Jay and James Madison\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.net\\n\\n\\nTitle: The Federalist Papers\\n\\nAuthor: Alexander Hamilton\\n        John Jay\\n        James Madison\\n\\nPosting Date: December 12, 2011 [EBook #18]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federalist[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the text file opens with a license agreement, hardly useful in our case. In fact, the license spans 113 words, while there is also a licensing agreement at the end of the file, which spans 3098 words. We need to remove them. To do so, we will first convert the text into words, to make our lives easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordseq = words(federalist)\n",
    "wordseq = wordseq[114:-3098]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the first 100 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'federalist no 1 general introduction for the independent journal hamilton to the people of the state of new york after an unequivocal experience of the inefficacy of the subsisting federal government you are called upon to deliberate on a new constitution for the united states of america the subject speaks its own importance comprehending in its consequences nothing less than the existence of the union the safety and welfare of the parts of which it is composed the fate of an empire in many respects the most interesting in the world it has been frequently remarked that it seems to'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(wordseq[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better.\n",
    "\n",
    "As with any Natural Language Processing problem, it is prudent to do some text pre-processing and clean our data before we start building our model. Remember that all the papers are signed as 'Publius', so we can safely remove that word, since it doesn't give us any information as to the real author.\n",
    "\n",
    "NOTE: Since we are only removing a single word from each paper, this step can be skipped. We add it here to show that processing the data in our hands is something we should always be considering. Oftentimes pre-processing the data in just the right way is the difference between a robust model and a flimsy one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordseq = [w for w in wordseq if w != 'publius']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to separate the text from a block of words into papers and assign them to their authors. We can see that each paper starts with the word 'federalist', so we will split the text on that word.\n",
    "\n",
    "The disputed papers are the papers from 49 to 58, from 18 to 20 and paper 64. We want to leave these papers unassigned. Also, note that there are two versions of paper 70; both from Hamilton.\n",
    "\n",
    "Finally, to keep the implementation intuitive, we add a `None` object at the start of the `papers` list to make the list index match up with the paper numbering (for example, `papers[5]` now corresponds to paper no. 5 instead of the paper no.6 in the 0-indexed Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16, 52)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "papers = re.split(r'federalist\\s', ' '.join(wordseq))\n",
    "papers = [p for p in papers if p not in ['', ' ']]\n",
    "papers = [None] + papers\n",
    "\n",
    "disputed = list(range(49, 58+1)) + [18, 19, 20, 64]\n",
    "jay, madison, hamilton = [], [], []\n",
    "for i, p in enumerate(papers):\n",
    "    if i in disputed or i == 0:\n",
    "        continue\n",
    "    \n",
    "    if 'jay' in p:\n",
    "        jay.append(p)\n",
    "    elif 'madison' in p:\n",
    "        madison.append(p)\n",
    "    else:\n",
    "        hamilton.append(p)\n",
    "\n",
    "len(jay), len(madison), len(hamilton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, from the undisputed papers Jay wrote 4, Madison 17 and Hamilton 51 (+1 duplicate). Let's now build our word models. The Unigram Word Model again will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamilton = ''.join(hamilton)\n",
    "hamilton_words = words(hamilton)\n",
    "P_hamilton = UnigramWordModel(hamilton_words, default=1)\n",
    "\n",
    "madison = ''.join(madison)\n",
    "madison_words = words(madison)\n",
    "P_madison = UnigramWordModel(madison_words, default=1)\n",
    "\n",
    "jay = ''.join(jay)\n",
    "jay_words = words(jay)\n",
    "P_jay = UnigramWordModel(jay_words, default=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build our new Naive Bayes Learner. It is very similar to the one found in `learning.py`, but with an important difference: it doesn't classify an example, but instead returns the probability of the example belonging to each class. This will allow us to not only see to whom a paper belongs to, but also the probability of authorship as well. \n",
    "We will build two versions of Learners, one will multiply probabilities as is and other will add the logarithms of them.\n",
    "\n",
    "Finally, since we are dealing with long text and the string of probability multiplications is long, we will end up with the results being rounded to 0 due to floating point underflow. To work around this problem we will use the built-in Python library `decimal`, which allows as to set decimal precision to much larger than normal.\n",
    "\n",
    "Note that the logarithmic learner will compute a negative likelihood since the logarithm of values less than 1 will be negative.\n",
    "Thus, the author with the lesser magnitude of proportion is more likely to have written that paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "from decimal import Decimal\n",
    "\n",
    "decimal.getcontext().prec = 100\n",
    "\n",
    "def precise_product(numbers):\n",
    "    result = 1\n",
    "    for x in numbers:\n",
    "        result *= Decimal(x)\n",
    "    return result\n",
    "\n",
    "def log_product(numbers):\n",
    "    result = 0.0\n",
    "    for x in numbers:\n",
    "        result += math.log(x)\n",
    "    return result\n",
    "\n",
    "def NaiveBayesLearner(dist):\n",
    "    \"\"\"A simple naive bayes classifier that takes as input a dictionary of\n",
    "    Counter distributions and can then be used to find the probability\n",
    "    of a given item belonging to each class.\n",
    "    The input dictionary is in the following form:\n",
    "        ClassName: Counter\"\"\"\n",
    "    attr_dist = {c_name: count_prob for c_name, count_prob in dist.items()}\n",
    "\n",
    "    def predict(example):\n",
    "        \"\"\"Predict the probabilities for each class.\"\"\"\n",
    "        def class_prob(target, e):\n",
    "            attr = attr_dist[target]\n",
    "            return precise_product([attr[a] for a in e])\n",
    "\n",
    "        pred = {t: class_prob(t, example) for t in dist.keys()}\n",
    "\n",
    "        total = sum(pred.values())\n",
    "        for k, v in pred.items():\n",
    "            pred[k] = v / total\n",
    "\n",
    "        return pred\n",
    "\n",
    "    return predict\n",
    "\n",
    "def NaiveBayesLearnerLog(dist):\n",
    "    \"\"\"A simple naive bayes classifier that takes as input a dictionary of\n",
    "    Counter distributions and can then be used to find the probability\n",
    "    of a given item belonging to each class. It will compute the likelihood by adding the logarithms of probabilities.\n",
    "    The input dictionary is in the following form:\n",
    "        ClassName: Counter\"\"\"\n",
    "    attr_dist = {c_name: count_prob for c_name, count_prob in dist.items()}\n",
    "\n",
    "    def predict(example):\n",
    "        \"\"\"Predict the probabilities for each class.\"\"\"\n",
    "        def class_prob(target, e):\n",
    "            attr = attr_dist[target]\n",
    "            return log_product([attr[a] for a in e])\n",
    "\n",
    "        pred = {t: class_prob(t, example) for t in dist.keys()}\n",
    "\n",
    "        total = -sum(pred.values())\n",
    "        for k, v in pred.items():\n",
    "            pred[k] = v/total\n",
    "\n",
    "        return pred\n",
    "\n",
    "    return predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build our Learner. Note that even though Hamilton wrote the most papers, that doesn't make it more probable that he wrote the rest, so all the class probabilities will be equal. We can change them if we have some external knowledge, which for this tutorial we do not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = {('Madison', 1): P_madison, ('Hamilton', 1): P_hamilton, ('Jay', 1): P_jay}\n",
    "nBS = NaiveBayesLearner(dist)\n",
    "nBSL = NaiveBayesLearnerLog(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the `recognize` function will take as input a string and after removing capitalization and splitting it into words, will feed it into the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize(sentence, nBS):\n",
    "    return nBS(words(sentence.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start predicting the disputed papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Straightforward Naive Bayes Learner\n",
      "\n",
      "Paper No. 49: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 50: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 51: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 52: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 53: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 54: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 55: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 56: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 57: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 58: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 18: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 19: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 20: Hamilton: 0.0000 Madison: 1.0000 Jay: 0.0000\n",
      "Paper No. 64: Hamilton: 1.0000 Madison: 0.0000 Jay: 0.0000\n",
      "\n",
      "Logarithmic Naive Bayes Learner\n",
      "\n",
      "Paper No. 49: Hamilton: -0.330591 Madison: -0.327717 Jay: -0.341692\n",
      "Paper No. 50: Hamilton: -0.333119 Madison: -0.328454 Jay: -0.338427\n",
      "Paper No. 51: Hamilton: -0.330246 Madison: -0.325758 Jay: -0.343996\n",
      "Paper No. 52: Hamilton: -0.331094 Madison: -0.327491 Jay: -0.341415\n",
      "Paper No. 53: Hamilton: -0.330942 Madison: -0.328364 Jay: -0.340693\n",
      "Paper No. 54: Hamilton: -0.329566 Madison: -0.327157 Jay: -0.343277\n",
      "Paper No. 55: Hamilton: -0.330821 Madison: -0.328143 Jay: -0.341036\n",
      "Paper No. 56: Hamilton: -0.330333 Madison: -0.327496 Jay: -0.342171\n",
      "Paper No. 57: Hamilton: -0.330625 Madison: -0.328602 Jay: -0.340772\n",
      "Paper No. 58: Hamilton: -0.330271 Madison: -0.327215 Jay: -0.342515\n",
      "Paper No. 18: Hamilton: -0.337781 Madison: -0.330932 Jay: -0.331287\n",
      "Paper No. 19: Hamilton: -0.335635 Madison: -0.331774 Jay: -0.332590\n",
      "Paper No. 20: Hamilton: -0.334911 Madison: -0.331866 Jay: -0.333223\n",
      "Paper No. 64: Hamilton: -0.331004 Madison: -0.332968 Jay: -0.336028\n"
     ]
    }
   ],
   "source": [
    "print('\\nStraightforward Naive Bayes Learner\\n')\n",
    "for d in disputed:\n",
    "    probs = recognize(papers[d], nBS)\n",
    "    results = ['{}: {:.4f}'.format(name, probs[(name, 1)]) for name in 'Hamilton Madison Jay'.split()]\n",
    "    print('Paper No. {}: {}'.format(d, ' '.join(results)))\n",
    "\n",
    "print('\\nLogarithmic Naive Bayes Learner\\n')\n",
    "for d in disputed:\n",
    "    probs = recognize(papers[d], nBSL)\n",
    "    results = ['{}: {:.6f}'.format(name, probs[(name, 1)]) for name in 'Hamilton Madison Jay'.split()]\n",
    "    print('Paper No. {}: {}'.format(d, ' '.join(results)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both learners classify the papers identically. Because of underflow in the straightforward learner, only one author remains with a positive value. The log learner is more accurate with marginal differences between all the authors. \n",
    "\n",
    "This is a simple approach to the problem and thankfully researchers are fairly certain that papers 49-58 were all written by Madison, while 18-20 were written in collaboration between Hamilton and Madison, with Madison being credited for most of the work. Our classifier is not that far off. It correctly identifies the papers written by Madison, even the ones in collaboration with Hamilton.\n",
    "\n",
    "Unfortunately, it misses paper 64. Consensus is that the paper was written by John Jay, while our classifier believes it was written by Hamilton. The classifier is wrong there because it does not have much information on Jay's writing; only 4 papers. This is one of the problems with using unbalanced datasets such as this one, where information on some classes is sparser than information on the rest. To avoid this, we can add more writings for Jay and Madison to end up with an equal amount of data for each author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis using TF-IDF\n",
    "Since we know that computers are great with numbers but they cannot work out with the natural language. So in-order to overcome that text can be directly converted to numbers for analyzing.One of the most common and popular technique for this is TF-IDF which stands for Term Frequency and Inverse Document Frequency. \n",
    "\n",
    "1. **TF(Term -Frequency):-**Gives the frequency of each word in a document. As the number of occurances of a word increases in a document its value increases for that document. Basically, if a word appears more times in a docuemnt then that word is important to that document. \n",
    "    \n",
    "    **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)**\n",
    "\n",
    "\n",
    "2. **IDF(Inverse Document Frequency):-** It calculates, how much a word is important for a given document. The words that occur in less documents are more important as compared to the words that occur in more number of documents. It helps to find the important words across the documents .\n",
    "\n",
    "    **IDF(t) = log(Total number of documents / (1+Number of documents with term t in it))**\n",
    "    \n",
    "    (1 is added to the base for coding purposes. It helps to deal with the cases when the term does not appears in any document)\n",
    "\n",
    "\n",
    "*TF-IDF* finally gives the importance to a single word in a collection of documents by multiplying the TF of that word in that document with the  IDF of that word  across the documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with TF-IDF  \n",
    "1. First of all we need a list of all the documents to work on.\n",
    "2. Next, basic text-procesing is done over all the documents.\n",
    "3. Now, an object is made of the class tf_idf_analyis with the list of all pre-processed text.\n",
    "4. We can find the term-frequency of a given document using *get_term_frequency(doc)*.\n",
    "5. The relevance of word can be calculated using *get_tf_idf_score(word,doc)* function. \n",
    "6. Using this object we can get the *n* most important words of the document using *top(doc,n)* function.\n",
    "7. We can see the tf-idf vector i.e. score of all the words in the document by *tf_idf_vector(doc)*.\n",
    "8. We can also search for a query by doing pre-text-processing on the text and then passing it on *search_query(query)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import open_data\n",
    "from text import *\n",
    "import re\n",
    "import math\n",
    "\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?\"\n",
    "docs=[document_0,document_1,document_2,document_3,document_4,document_5,document_6]\n",
    "for i in range(len(docs)):\n",
    "    docs[i]=' '.join(words(docs[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf_analysis:\n",
    "    '''a class to perform tf-idf analysis on a given set of documents and also search a query'''\n",
    "#     class variabels(their type)= values contained in the variable\n",
    "#     docs(list)=contains all the documents\n",
    "#     terms_tf_idf_score(list of dict)=tf-idf-score of all the documents with all words\n",
    "#     doc_tf(list of dict)=a list containting the tf of each word in each document\n",
    "#     terms_df(dict of list)= gives the df of ach term\n",
    "    def __init__(self,docs):\n",
    "        '''input: list of all documents'''\n",
    "        self.docs=docs\n",
    "        self.tf_idf_score=self.make_tf_idf()\n",
    "    def make_tf_idf(self):\n",
    "        '''makes the tf-idf score of all the words in all the documents'''\n",
    "        terms_df={}\n",
    "        doc_tf=[]\n",
    "        counter=0\n",
    "        for i in self.docs:\n",
    "            counter+=1\n",
    "            tf=self.get_term_frequency(i)\n",
    "            doc_tf.append(tf)\n",
    "            for j in tf.items():\n",
    "                if(terms_df.get(j[0],None)==None):\n",
    "                    terms_df[j[0]]=[counter]\n",
    "                elif(i not in terms_df[j[0]]):\n",
    "                    terms_df[j[0]].append(counter)\n",
    "        terms_tf_idf_score=[]\n",
    "        for i in doc_tf:\n",
    "            x={}\n",
    "            for j in i.items():\n",
    "                x[j[0]]=self.tf_mul_idf(j[1],len(terms_df.get(j[0],[])))\n",
    "            terms_tf_idf_score.append(x)\n",
    "        self.terms_tf_idf_score=terms_tf_idf_score\n",
    "        self.doc_tf=doc_tf\n",
    "        self.terms_df=terms_df\n",
    "        return terms_tf_idf_score\n",
    "    def tf_mul_idf(self,tf,df):\n",
    "        '''multiplies the term frequency and inter-docuemt frequency to give the correct score'''\n",
    "        return math.log(1+tf)*math.log(len(self.docs)/(1+df))\n",
    "    def get_term_frequency(self,doc):\n",
    "        '''returns a dictionary containing the frequency of each term in a given document'''\n",
    "        term_freq={}\n",
    "        total_terms=0\n",
    "        for i in doc.split():\n",
    "            total_terms+=1\n",
    "            if(term_freq.get(i,0)==0):\n",
    "                term_freq[i]=1\n",
    "            else:\n",
    "                term_freq[i]+=1\n",
    "        for i,j in term_freq.items():\n",
    "            term_freq[i]=j/total_terms\n",
    "        return term_freq\n",
    "    def get_doc_no(self,doc):\n",
    "        '''gets the document number of a given document from the list of docuemnts given '''\n",
    "        counter=0\n",
    "        for i in self.docs:\n",
    "            if(i==doc):\n",
    "                return counter\n",
    "            counter+=1\n",
    "    def get_tf_idf_score(self,word,doc):\n",
    "        '''returns the tf-idf score of the document '''\n",
    "        tf=self.get_term_frequency(doc)\n",
    "        ans=self.tf_mul_idf(tf.get(word,0),len(self.terms_df.get(word,[])))\n",
    "        return ans\n",
    "    def top(self,doc_no,n):\n",
    "        '''returns top n most important word of the given document number'''\n",
    "#         doc_no=self.get_doc_no(doc)\n",
    "        x=self.terms_tf_idf_score[doc_no]\n",
    "        return sorted(x.items(),key=lambda z:z[1],reverse=True)[:n]\n",
    "    def get_tf_idf_vector(self,doc):\n",
    "        '''returns the tf-idf vector of the document'''\n",
    "        tf=self.get_term_frequency(doc)\n",
    "        vector={}\n",
    "        for i in doc.split():\n",
    "            vector[i]=self.tf_mul_idf(tf.get(i,0),len(self.terms_df.get(i,[])))\n",
    "        return vector\n",
    "    def search_query(self,query):\n",
    "        '''returns the sorted list of all the relevant docuemnts along with their relevance scores'''\n",
    "        queryvec=self.get_tf_idf_vector(query)\n",
    "        doc_score={}\n",
    "        query_mag=math.sqrt(sum([j[1]**2 for j in queryvec.items()]))\n",
    "        for i in range(len(self.docs)):\n",
    "            common_words=[j for j in query.split() if j in self.docs[i].split()]\n",
    "            if(len(common_words)==0):\n",
    "                doc_score[i]=0\n",
    "            else:\n",
    "                dot_product=sum([queryvec[j]*self.terms_tf_idf_score[i][j] for j in common_words])\n",
    "                doc_mag=math.sqrt(sum([j[1]**2 for j in self.terms_tf_idf_score[i].items()]))\n",
    "                doc_score[i]=dot_product/(query_mag*doc_mag)\n",
    "        sorted_doc_score=sorted(doc_score.items(),key=lambda x:x[1],reverse=True)\n",
    "        return sorted_doc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance of the word \"in\" in docs[1]:-  0.0864317144890127\n",
      "0.18906905358773213\n"
     ]
    }
   ],
   "source": [
    "x=tf_idf_analysis(docs)\n",
    "word=\"in\"\n",
    "doc_no=1\n",
    "print(\"relevance of the word \\\"\"+word+\"\\\" in docs[\"+str(doc_no)+\"]:- \",x.get_tf_idf_score(\"corruption\",docs[doc_no]))\n",
    "# this also works with any other random document\n",
    "new_doc=\"corruption in this world.\"\n",
    "print(x.get_tf_idf_score(word,new_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'china': 0.047619047619047616, 'has': 0.047619047619047616, 'a': 0.09523809523809523, 'strong': 0.047619047619047616, 'economy': 0.09523809523809523, 'that': 0.047619047619047616, 'is': 0.047619047619047616, 'growing': 0.047619047619047616, 'at': 0.047619047619047616, 'rapid': 0.047619047619047616, 'pace': 0.047619047619047616, 'however': 0.047619047619047616, 'politically': 0.047619047619047616, 'it': 0.047619047619047616, 'differs': 0.047619047619047616, 'greatly': 0.047619047619047616, 'from': 0.047619047619047616, 'the': 0.047619047619047616, 'us': 0.047619047619047616}\n"
     ]
    }
   ],
   "source": [
    "# to get the normalized term frequency of all the words in the docuemnt\n",
    "print(x.get_term_frequency(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0.07708019302933156),\n",
       " ('strong', 0.05827855288121937),\n",
       " ('that', 0.05827855288121937),\n",
       " ('growing', 0.05827855288121937),\n",
       " ('rapid', 0.05827855288121937),\n",
       " ('pace', 0.05827855288121937),\n",
       " ('however', 0.05827855288121937),\n",
       " ('politically', 0.05827855288121937),\n",
       " ('it', 0.05827855288121937),\n",
       " ('differs', 0.05827855288121937)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also see the top 10 most important words to that document \n",
    "x.top(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at': 0.05845751239286325,\n",
       " 'last': 0.0864317144890127,\n",
       " 'china': 0.05845751239286325,\n",
       " 'seems': 0.05845751239286325,\n",
       " 'serious': 0.05845751239286325,\n",
       " 'about': 0.05845751239286325,\n",
       " 'confronting': 0.0864317144890127,\n",
       " 'an': 0.0864317144890127,\n",
       " 'endemic': 0.0864317144890127,\n",
       " 'problem': 0.0864317144890127,\n",
       " 'domestic': 0.0864317144890127,\n",
       " 'violence': 0.0864317144890127,\n",
       " 'and': 0.0864317144890127,\n",
       " 'corruption': 0.0864317144890127}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the tf-idf vector of the complete sentence  \n",
    "x.get_tf_idf_vector(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Search using TF-IDF\n",
    "For searching a query in a list of documents, a *relevance score* for each document is calculated. This is calucated by :-\n",
    "1. Finding the common words in the query and the document. \n",
    "2. Then the tf-idf score of the common words in the query and the document are multiplied.\n",
    "3. Then the cosine simmilarity is calculated b/w the query and that document by multiplying the tf-idf score of the common words and then dividing them by the magnitude of both the vectors.\n",
    "This can be implemented using *search_query(query)* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0.4257678912782361), (3, 0.3325447850032575), (0, 0), (1, 0), (2, 0), (4, 0), (5, 0)]\n"
     ]
    }
   ],
   "source": [
    "query=\"Vladimir putin\"\n",
    "results=x.search_query(\" \".join(words(query)))\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
